{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "users = [\n",
        "    {\"name\": \"Alex\", \"country\": \"India\", \"active\": True},\n",
        "    {\"name\": \"Bob\", \"country\": \"USA\", \"active\": False},\n",
        "    {\"name\": \"Charlie\", \"country\": \"UK\", \"active\": True}\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "bXSdYBYLY2H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for user in users:\n",
        "    print(user)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp4HPqROZGje",
        "outputId": "9aec3ca5-ed12-4632-c208-9fbe6b0b28eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Alex', 'country': 'India', 'active': True}\n",
            "{'name': 'Bob', 'country': 'USA', 'active': False}\n",
            "{'name': 'Charlie', 'country': 'UK', 'active': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for user in users :\n",
        "  print(user[\"name\"], \"from\", user[\"country\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73x1ink2ZeY2",
        "outputId": "b005d55f-0e82-41e6-88b0-d711c30337d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alex from India\n",
            "Bob from USA\n",
            "Charlie from UK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_user = users[0]\n",
        "\n",
        "for key, value in sample_user.items():\n",
        "    print(key, \"=>\", value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaPAHMZEapzU",
        "outputId": "16daca2e-6ec6-42e7-ed26-e169f7810a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name => Alex\n",
            "country => India\n",
            "active => True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in sample_user.keys():\n",
        "  print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asMlhZVlbArt",
        "outputId": "151c18eb-84a2-4295-8482-f5d5a07fa1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name\n",
            "country\n",
            "active\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for value in sample_user.values() :\n",
        "  print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmM0xDpRbZoJ",
        "outputId": "3cbffdd6-d1b2-4c7f-b83c-ae33b4503a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alex\n",
            "India\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in sample_user.items() :\n",
        "  print(key, \":\", value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFUnbTgRbrmi",
        "outputId": "7f7ae60e-ee4e-4504-d536-3dca130fadee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name : Alex\n",
            "country : India\n",
            "active : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "## Why is a dictionary ideal for API data?\n",
        "# APIs often return data with varying fields. Dictionaries handle this naturally\n",
        "# APIs commonly use JSON format, which maps directly to Python dictionaries\n",
        "\n",
        "## What happens if a key is missing?\n",
        "# when a key is missing, there is direct access through KeyError as well as safe access methods with .get() or try except blocks\n"
      ],
      "metadata": {
        "id": "qVAxdcudb_dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### M1.03#######################################\n",
        "\n",
        "users = [\n",
        "    {\"name\": \"Alex\", \"age\": 28, \"email\": \"alex@example.com\"},\n",
        "    {\"name\": \"\", \"age\": 35, \"email\": \"bob@example.com\"},\n",
        "    {\"name\": \"Charlie\", \"age\": -2, \"email\": \"charlieexample.com\"},\n",
        "    {\"name\": \"Diana\", \"age\": 22}\n",
        "]\n"
      ],
      "metadata": {
        "id": "kT2sYxihNnaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, user in enumerate(users, 1):\n",
        "    print(f\"\\nUser {i}: {user}\")\n",
        "    errors = []\n",
        "\n",
        "    # Check name\n",
        "    if 'name' not in user or not user['name'] or user['name'].strip() == '':\n",
        "        errors.append(\"‚ùå Name is missing or empty\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Name: '{user['name']}'\")\n",
        "\n",
        "    # Check age\n",
        "    if 'age' not in user:\n",
        "        errors.append(\"‚ùå Age field is missing\")\n",
        "    elif user['age'] <= 0:\n",
        "        errors.append(f\"‚ùå Age must be > 0, got {user['age']}\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Age: {user['age']}\")\n",
        "\n",
        "    # Check email\n",
        "    if 'email' not in user:\n",
        "        errors.append(\"‚ùå Email field is missing\")\n",
        "    elif '@' not in user['email']:\n",
        "        errors.append(f\"‚ùå Email must contain '@', got '{user['email']}'\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Email: {user['email']}\")\n",
        "\n",
        "    # Show errors if any\n",
        "    if errors:\n",
        "        print(\"  Issues found:\")\n",
        "        for error in errors:\n",
        "            print(f\"    {error}\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ All validations passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iZbaNjFN3BF",
        "outputId": "5017e340-fdd7-499e-ece2-555dd6f4c90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User 1: {'name': 'Alex', 'age': 28, 'email': 'alex@example.com'}\n",
            "  ‚úÖ Name: 'Alex'\n",
            "  ‚úÖ Age: 28\n",
            "  ‚úÖ Email: alex@example.com\n",
            "  ‚úÖ All validations passed!\n",
            "\n",
            "User 2: {'name': '', 'age': 35, 'email': 'bob@example.com'}\n",
            "  ‚úÖ Age: 35\n",
            "  ‚úÖ Email: bob@example.com\n",
            "  Issues found:\n",
            "    ‚ùå Name is missing or empty\n",
            "\n",
            "User 3: {'name': 'Charlie', 'age': -2, 'email': 'charlieexample.com'}\n",
            "  ‚úÖ Name: 'Charlie'\n",
            "  Issues found:\n",
            "    ‚ùå Age must be > 0, got -2\n",
            "    ‚ùå Email must contain '@', got 'charlieexample.com'\n",
            "\n",
            "User 4: {'name': 'Diana', 'age': 22}\n",
            "  ‚úÖ Name: 'Diana'\n",
            "  ‚úÖ Age: 22\n",
            "  Issues found:\n",
            "    ‚ùå Email field is missing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for user in users:\n",
        "    errors = []\n",
        "\n",
        "    if \"name\" not in user or user[\"name\"] == \"\":\n",
        "        errors.append(\"name\")\n",
        "\n",
        "    if \"age\" not in user or user[\"age\"] <= 0:\n",
        "        errors.append(\"age\")\n",
        "\n",
        "    if \"email\" not in user or \"@\" not in user[\"email\"]:\n",
        "        errors.append(\"email\")\n",
        "\n",
        "    if errors:\n",
        "        print(user.get(\"name\", \"Unknown\"), \"‚ùå INVALID:\", errors)\n",
        "    else:\n",
        "        print(user[\"name\"], \"‚úÖ VALID\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzm-FXLaOFt8",
        "outputId": "db24c37c-d69f-4f58-b4ad-60ad4c2751bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alex ‚úÖ VALID\n",
            " ‚ùå INVALID: ['name']\n",
            "Charlie ‚ùå INVALID: ['age', 'email']\n",
            "Diana ‚ùå INVALID: ['email']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Why is validation important?\n",
        "# validation is important because garbage in , garbage out. This have can have hard consequences in real life sectors like healthcare and finance\n",
        "\n",
        "### What could go wrong if we skip this step?\n",
        "# the system could crash, one could do wrong analytics and one could also end up beeing regulatory non - complaint"
      ],
      "metadata": {
        "id": "fErYwhxBOLA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Create the CSV file using Python\n",
        "content = \"\"\"\n",
        "transaction_id,customer_id,amount,currency,status\n",
        "T1001,C001,250,USD,COMPLETED\n",
        "T1002,C002,,USD,PENDING\n",
        "T1003,,450,EUR,FAILED\n",
        "T1004,C004,-50,USD,COMPLETED\n",
        "T1005,C005,300,,COMPLETED\n",
        "\"\"\"\n",
        "\n",
        "with open(\"transactions.csv\", \"w\") as file:\n",
        "    file.write(content)\n",
        "\n",
        "print(\"transactions.csv created successfully!\")\n"
      ],
      "metadata": {
        "id": "fRpAMwneI3Yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd744d32-8608-4f29-81d3-fe981115aa88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transactions.csv created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "report = {\n",
        "    \"row_count\": 0,\n",
        "    \"columns\": [],\n",
        "    \"missing_values\": {},\n",
        "    \"invalid_values\": {}\n",
        "}\n"
      ],
      "metadata": {
        "id": "nmDqnp0ycgID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import sys\n",
        "import datetime\n",
        "import argparse\n",
        "import os\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass, asdict\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "class SeverityLevel(Enum):\n",
        "    \"\"\"Severity levels for quality issues.\"\"\"\n",
        "    LOW = \"low\"\n",
        "    MEDIUM = \"medium\"\n",
        "    HIGH = \"high\"\n",
        "    CRITICAL = \"critical\"\n",
        "\n",
        "\n",
        "class ColumnType(Enum):\n",
        "    \"\"\"Data types for columns.\"\"\"\n",
        "    INTEGER = \"integer\"\n",
        "    FLOAT = \"float\"\n",
        "    STRING = \"string\"\n",
        "    BOOLEAN = \"boolean\"\n",
        "    DATE = \"date\"\n",
        "    EMAIL = \"email\"\n",
        "    UNKNOWN = \"unknown\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class QualityMetric:\n",
        "    \"\"\"Data class for storing quality metrics.\"\"\"\n",
        "    total_rows: int = 0\n",
        "    total_columns: int = 0\n",
        "    complete_rows: int = 0\n",
        "    empty_cells: int = 0\n",
        "    duplicate_rows: int = 0\n",
        "    invalid_values: int = 0\n",
        "    quality_score: float = 0.0\n",
        "    processing_time: float = 0.0\n",
        "\n",
        "\n",
        "class CSVQualityAnalyzer:\n",
        "    \"\"\"\n",
        "    Main analyzer class for CSV quality assessment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with a CSV file path.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the CSV file to analyze\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "        self.file_name = os.path.basename(file_path)\n",
        "        self.file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0\n",
        "        self.report = {}\n",
        "        self.metrics = QualityMetric()\n",
        "        self.start_time = datetime.datetime.now()\n",
        "\n",
        "    def analyze(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Perform comprehensive analysis of the CSV file.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing the complete quality report\n",
        "        \"\"\"\n",
        "        print(f\"\\nüîç Ironhack CSV Quality Analyzer\")\n",
        "        print(f\"üìÅ Analyzing: {self.file_name}\")\n",
        "        print(f\"‚è±Ô∏è  Started at: {self.start_time.strftime('%H:%M:%S')}\")\n",
        "        print(\"‚îÄ\" * 50)\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, 'r', encoding='utf-8') as file:\n",
        "                reader = csv.DictReader(file)\n",
        "\n",
        "                # Basic file validation\n",
        "                if not reader.fieldnames:\n",
        "                    raise ValueError(\"CSV file has no headers\")\n",
        "\n",
        "                # Initialize data structures\n",
        "                columns = reader.fieldnames\n",
        "                self._initialize_analysis(columns)\n",
        "\n",
        "                # Process all rows\n",
        "                self._process_rows(reader)\n",
        "\n",
        "                # Calculate derived metrics\n",
        "                self._calculate_metrics()\n",
        "\n",
        "                # Generate final report\n",
        "                self._generate_report()\n",
        "\n",
        "                # Calculate processing time\n",
        "                self.metrics.processing_time = (\n",
        "                    datetime.datetime.now() - self.start_time\n",
        "                ).total_seconds()\n",
        "\n",
        "                return self.report\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            self._handle_error(\"File not found. Please check the file path.\")\n",
        "        except UnicodeDecodeError:\n",
        "            self._handle_error(\"File encoding issue. Try UTF-8 or specify encoding.\")\n",
        "        except csv.Error as e:\n",
        "            self._handle_error(f\"CSV parsing error: {str(e)}\")\n",
        "        except Exception as e:\n",
        "            self._handle_error(f\"Unexpected error: {str(e)}\")\n",
        "\n",
        "        return self.report\n",
        "\n",
        "    def _initialize_analysis(self, columns: List[str]) -> None:\n",
        "        \"\"\"Initialize data structures for analysis.\"\"\"\n",
        "        self.columns = columns\n",
        "        self.metrics.total_columns = len(columns)\n",
        "\n",
        "        # Initialize counters\n",
        "        self.row_data = []\n",
        "        self.missing_counts = {col: 0 for col in columns}\n",
        "        self.invalid_counts = defaultdict(int)\n",
        "        self.type_counts = {col: defaultdict(int) for col in columns}\n",
        "        self.seen_hashes = set()\n",
        "        self.duplicate_rows = []\n",
        "\n",
        "        # Initialize column metadata\n",
        "        self.column_metadata = {\n",
        "            col: {\n",
        "                'type_guesses': defaultdict(int),\n",
        "                'min_length': float('inf'),\n",
        "                'max_length': 0,\n",
        "                'unique_values': set(),\n",
        "                'numeric_stats': {'sum': 0, 'count': 0, 'min': float('inf'), 'max': float('-inf')}\n",
        "            }\n",
        "            for col in columns\n",
        "        }\n",
        "\n",
        "    def _process_rows(self, reader: csv.DictReader) -> None:\n",
        "        \"\"\"Process all rows in the CSV file.\"\"\"\n",
        "        for row_idx, row in enumerate(reader, start=1):\n",
        "            self.metrics.total_rows += 1\n",
        "\n",
        "            # Track progress\n",
        "            if row_idx % 1000 == 0:\n",
        "                print(f\"  Processed {row_idx:,} rows...\")\n",
        "\n",
        "            # Create row hash for duplicate detection\n",
        "            row_hash = self._create_row_hash(row)\n",
        "\n",
        "            # Check for duplicates\n",
        "            if row_hash in self.seen_hashes:\n",
        "                self.metrics.duplicate_rows += 1\n",
        "                self.duplicate_rows.append(row_idx)\n",
        "            else:\n",
        "                self.seen_hashes.add(row_hash)\n",
        "                self.row_data.append(row)\n",
        "\n",
        "            # Analyze each column in the row\n",
        "            self._analyze_row(row, row_idx)\n",
        "\n",
        "        print(f\"  ‚úì Completed processing {self.metrics.total_rows:,} rows\")\n",
        "\n",
        "    def _analyze_row(self, row: Dict, row_idx: int) -> None:\n",
        "        \"\"\"Analyze a single row.\"\"\"\n",
        "        row_complete = True\n",
        "\n",
        "        for col in self.columns:\n",
        "            value = str(row.get(col, \"\")).strip()\n",
        "\n",
        "            # Check for missing values\n",
        "            if not value or value.lower() in ['null', 'nan', 'none', '']:\n",
        "                self.missing_counts[col] += 1\n",
        "                row_complete = False\n",
        "                continue\n",
        "\n",
        "            # Analyze non-missing values\n",
        "            self._analyze_value(col, value, row_idx)\n",
        "\n",
        "            # Update column metadata\n",
        "            self._update_column_metadata(col, value)\n",
        "\n",
        "        if row_complete:\n",
        "            self.metrics.complete_rows += 1\n",
        "\n",
        "    def _analyze_value(self, col: str, value: str, row_idx: int) -> None:\n",
        "        \"\"\"Analyze a single value.\"\"\"\n",
        "        # Guess data type\n",
        "        guessed_type = self._guess_data_type(value)\n",
        "        self.type_counts[col][guessed_type] += 1\n",
        "\n",
        "        # Column-specific validation\n",
        "        self._validate_column_specific(col, value, row_idx)\n",
        "\n",
        "        # Length analysis\n",
        "        value_length = len(value)\n",
        "        if value_length < self.column_metadata[col]['min_length']:\n",
        "            self.column_metadata[col]['min_length'] = value_length\n",
        "        if value_length > self.column_metadata[col]['max_length']:\n",
        "            self.column_metadata[col]['max_length'] = value_length\n",
        "\n",
        "        # Track unique values\n",
        "        self.column_metadata[col]['unique_values'].add(value)\n",
        "\n",
        "    def _guess_data_type(self, value: str) -> ColumnType:\n",
        "        \"\"\"Guess the data type of a value.\"\"\"\n",
        "        try:\n",
        "            int(value)\n",
        "            return ColumnType.INTEGER\n",
        "        except ValueError:\n",
        "            try:\n",
        "                float(value)\n",
        "                return ColumnType.FLOAT\n",
        "            except ValueError:\n",
        "                if value.lower() in ['true', 'false', 'yes', 'no', '1', '0']:\n",
        "                    return ColumnType.BOOLEAN\n",
        "                elif '@' in value and '.' in value:\n",
        "                    return ColumnType.EMAIL\n",
        "                elif any(sep in value for sep in ['-', '/', '.']) and len(value) >= 8:\n",
        "                    return ColumnType.DATE\n",
        "                else:\n",
        "                    return ColumnType.STRING\n",
        "\n",
        "    def _validate_column_specific(self, col: str, value: str, row_idx: int) -> None:\n",
        "        \"\"\"Perform column-specific validation.\"\"\"\n",
        "        col_lower = col.lower()\n",
        "\n",
        "        # Email validation\n",
        "        if 'email' in col_lower:\n",
        "            if '@' not in value or '.' not in value or len(value.split('@')) != 2:\n",
        "                self.invalid_counts[f\"{col}_invalid_email\"] += 1\n",
        "\n",
        "        # Amount/price validation\n",
        "        elif any(keyword in col_lower for keyword in ['amount', 'price', 'cost', 'total']):\n",
        "            try:\n",
        "                num_val = float(value)\n",
        "                if num_val < 0:\n",
        "                    self.invalid_counts[f\"{col}_negative\"] += 1\n",
        "                elif num_val == 0:\n",
        "                    self.invalid_counts[f\"{col}_zero\"] += 1\n",
        "            except ValueError:\n",
        "                self.invalid_counts[f\"{col}_non_numeric\"] += 1\n",
        "\n",
        "        # Date validation\n",
        "        elif any(keyword in col_lower for keyword in ['date', 'time', 'timestamp']):\n",
        "            # Simple date format check\n",
        "            date_parts = value.replace('/', '-').replace('.', '-').split('-')\n",
        "            if len(date_parts) < 2 or not all(part.strip() for part in date_parts):\n",
        "                self.invalid_counts[f\"{col}_invalid_date\"] += 1\n",
        "\n",
        "        # ID validation\n",
        "        elif col_lower.endswith('_id') or col_lower == 'id':\n",
        "            if not value.isalnum():\n",
        "                self.invalid_counts[f\"{col}_invalid_id\"] += 1\n",
        "\n",
        "    def _update_column_metadata(self, col: str, value: str) -> None:\n",
        "        \"\"\"Update column metadata with value information.\"\"\"\n",
        "        # Update type guesses\n",
        "        guessed_type = self._guess_data_type(value)\n",
        "        self.column_metadata[col]['type_guesses'][guessed_type] += 1\n",
        "\n",
        "        # Update numeric stats if applicable\n",
        "        if guessed_type in [ColumnType.INTEGER, ColumnType.FLOAT]:\n",
        "            try:\n",
        "                num_val = float(value)\n",
        "                stats = self.column_metadata[col]['numeric_stats']\n",
        "                stats['sum'] += num_val\n",
        "                stats['count'] += 1\n",
        "                stats['min'] = min(stats['min'], num_val)\n",
        "                stats['max'] = max(stats['max'], num_val)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "    def _create_row_hash(self, row: Dict) -> int:\n",
        "        \"\"\"Create a hash for a row to detect duplicates.\"\"\"\n",
        "        row_str = '|'.join(str(row.get(col, '')).strip() for col in self.columns)\n",
        "        return hash(row_str)\n",
        "\n",
        "    def _calculate_metrics(self) -> None:\n",
        "        \"\"\"Calculate all derived metrics.\"\"\"\n",
        "        # Calculate missing values\n",
        "        self.metrics.empty_cells = sum(self.missing_counts.values())\n",
        "\n",
        "        # Calculate invalid values\n",
        "        self.metrics.invalid_values = sum(self.invalid_counts.values())\n",
        "\n",
        "        # Calculate quality score (0-100)\n",
        "        total_cells = self.metrics.total_rows * self.metrics.total_columns\n",
        "\n",
        "        if total_cells > 0:\n",
        "            completeness_score = (1 - self.metrics.empty_cells / total_cells) * 40\n",
        "            uniqueness_score = (1 - self.metrics.duplicate_rows / max(1, self.metrics.total_rows)) * 30\n",
        "            validity_score = (1 - self.metrics.invalid_values / max(1, total_cells - self.metrics.empty_cells)) * 30\n",
        "\n",
        "            self.metrics.quality_score = min(100, max(0,\n",
        "                completeness_score + uniqueness_score + validity_score))\n",
        "        else:\n",
        "            self.metrics.quality_score = 0\n",
        "\n",
        "    def _generate_report(self) -> None:\n",
        "        \"\"\"Generate the final quality report.\"\"\"\n",
        "        self.report = {\n",
        "            'metadata': {\n",
        "                'file_name': self.file_name,\n",
        "                'file_path': self.file_path,\n",
        "                'file_size_bytes': self.file_size,\n",
        "                'analysis_date': datetime.datetime.now().isoformat(),\n",
        "                'processing_time_seconds': self.metrics.processing_time\n",
        "            },\n",
        "            'summary': {\n",
        "                'total_rows': self.metrics.total_rows,\n",
        "                'total_columns': self.metrics.total_columns,\n",
        "                'complete_rows': self.metrics.complete_rows,\n",
        "                'completeness_percentage': round(\n",
        "                    (self.metrics.complete_rows / self.metrics.total_rows * 100)\n",
        "                    if self.metrics.total_rows > 0 else 0, 2\n",
        "                ),\n",
        "                'quality_score': round(self.metrics.quality_score, 2),\n",
        "                'quality_rating': self._get_quality_rating(self.metrics.quality_score)\n",
        "            },\n",
        "            'column_analysis': self._generate_column_analysis(),\n",
        "            'issues': self._generate_issues_report(),\n",
        "            'recommendations': self._generate_recommendations()\n",
        "        }\n",
        "\n",
        "    def _generate_column_analysis(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate detailed column analysis.\"\"\"\n",
        "        analysis = {}\n",
        "\n",
        "        for col in self.columns:\n",
        "            missing_pct = (self.missing_counts[col] / self.metrics.total_rows * 100) \\\n",
        "                if self.metrics.total_rows > 0 else 0\n",
        "\n",
        "            # Determine most likely type\n",
        "            if self.type_counts[col]:\n",
        "                most_common_type = max(self.type_counts[col].items(), key=lambda x: x[1])[0]\n",
        "                type_confidence = (\n",
        "                    self.type_counts[col][most_common_type] /\n",
        "                    (self.metrics.total_rows - self.missing_counts[col]) * 100\n",
        "                ) if (self.metrics.total_rows - self.missing_counts[col]) > 0 else 0\n",
        "            else:\n",
        "                most_common_type = ColumnType.UNKNOWN\n",
        "                type_confidence = 0\n",
        "\n",
        "            analysis[col] = {\n",
        "                'missing_count': self.missing_counts[col],\n",
        "                'missing_percentage': round(missing_pct, 2),\n",
        "                'completeness_severity': self._get_severity(missing_pct),\n",
        "                'data_type': most_common_type.value,\n",
        "                'type_confidence': round(type_confidence, 2),\n",
        "                'unique_values': len(self.column_metadata[col]['unique_values']),\n",
        "                'value_length': {\n",
        "                    'min': self.column_metadata[col]['min_length']\n",
        "                    if self.column_metadata[col]['min_length'] != float('inf') else 0,\n",
        "                    'max': self.column_metadata[col]['max_length']\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Add numeric stats if applicable\n",
        "            if most_common_type in [ColumnType.INTEGER, ColumnType.FLOAT]:\n",
        "                stats = self.column_metadata[col]['numeric_stats']\n",
        "                if stats['count'] > 0:\n",
        "                    analysis[col]['numeric_stats'] = {\n",
        "                        'count': stats['count'],\n",
        "                        'sum': round(stats['sum'], 2),\n",
        "                        'average': round(stats['sum'] / stats['count'], 2),\n",
        "                        'min': round(stats['min'], 2),\n",
        "                        'max': round(stats['max'], 2),\n",
        "                        'range': round(stats['max'] - stats['min'], 2)\n",
        "                    }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _generate_issues_report(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate report of identified issues.\"\"\"\n",
        "        issues = {\n",
        "            'duplicates': {\n",
        "                'count': self.metrics.duplicate_rows,\n",
        "                'percentage': round(\n",
        "                    (self.metrics.duplicate_rows / self.metrics.total_rows * 100)\n",
        "                    if self.metrics.total_rows > 0 else 0, 2\n",
        "                ),\n",
        "                'rows': self.duplicate_rows[:100]  # Limit to first 100 for readability\n",
        "            },\n",
        "            'invalid_values': dict(self.invalid_counts),\n",
        "            'columns_with_issues': []\n",
        "        }\n",
        "\n",
        "        # Identify problematic columns\n",
        "        for col in self.columns:\n",
        "            missing_pct = (self.missing_counts[col] / self.metrics.total_rows * 100) \\\n",
        "                if self.metrics.total_rows > 0 else 0\n",
        "\n",
        "            if missing_pct > 20 or col in self.invalid_counts:\n",
        "                issues['columns_with_issues'].append({\n",
        "                    'column': col,\n",
        "                    'missing_percentage': round(missing_pct, 2),\n",
        "                    'severity': self._get_severity(missing_pct)\n",
        "                })\n",
        "\n",
        "        return issues\n",
        "\n",
        "    def _generate_recommendations(self) -> List[str]:\n",
        "        \"\"\"Generate actionable recommendations.\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # Completeness recommendations\n",
        "        incomplete_cols = [\n",
        "            col for col, count in self.missing_counts.items()\n",
        "            if count / self.metrics.total_rows > 0.2\n",
        "        ]\n",
        "        if incomplete_cols:\n",
        "            recommendations.append(\n",
        "                f\"Consider data imputation for columns with >20% missing values: \"\n",
        "                f\"{', '.join(incomplete_cols[:5])}{'...' if len(incomplete_cols) > 5 else ''}\"\n",
        "            )\n",
        "\n",
        "        # Duplicate recommendations\n",
        "        if self.metrics.duplicate_rows > 0:\n",
        "            dup_pct = (self.metrics.duplicate_rows / self.metrics.total_rows * 100)\n",
        "            recommendations.append(\n",
        "                f\"Remove {self.metrics.duplicate_rows} duplicate rows \"\n",
        "                f\"({dup_pct:.1f}% of total) to improve data quality\"\n",
        "            )\n",
        "\n",
        "        # Type consistency recommendations\n",
        "        for col in self.columns:\n",
        "            if self.type_counts[col]:\n",
        "                total = sum(self.type_counts[col].values())\n",
        "                if total > 0:\n",
        "                    main_type = max(self.type_counts[col].items(), key=lambda x: x[1])[0]\n",
        "                    type_pct = (self.type_counts[col][main_type] / total * 100)\n",
        "                    if type_pct < 90:\n",
        "                        recommendations.append(\n",
        "                            f\"Column '{col}' has mixed data types. \"\n",
        "                            f\"Consider standardizing to {main_type.value}\"\n",
        "                        )\n",
        "\n",
        "        # Overall quality recommendation\n",
        "        if self.metrics.quality_score < 70:\n",
        "            recommendations.append(\n",
        "                f\"Overall data quality needs improvement. \"\n",
        "                f\"Focus on reducing missing values and fixing invalid entries.\"\n",
        "            )\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def _get_severity(self, percentage: float) -> str:\n",
        "        \"\"\"Get severity level based on percentage.\"\"\"\n",
        "        if percentage == 0:\n",
        "            return SeverityLevel.LOW.value\n",
        "        elif percentage <= 5:\n",
        "            return SeverityLevel.LOW.value\n",
        "        elif percentage <= 20:\n",
        "            return SeverityLevel.MEDIUM.value\n",
        "        elif percentage <= 50:\n",
        "            return SeverityLevel.HIGH.value\n",
        "        else:\n",
        "            return SeverityLevel.CRITICAL.value\n",
        "\n",
        "    def _get_quality_rating(self, score: float) -> str:\n",
        "        \"\"\"Get quality rating based on score.\"\"\"\n",
        "        if score >= 90:\n",
        "            return \"Excellent üèÜ\"\n",
        "        elif score >= 75:\n",
        "            return \"Good üëç\"\n",
        "        elif score >= 60:\n",
        "            return \"Fair ‚ö†Ô∏è\"\n",
        "        elif score >= 40:\n",
        "            return \"Poor üëé\"\n",
        "        else:\n",
        "            return \"Critical üö®\"\n",
        "\n",
        "    def _handle_error(self, message: str) -> None:\n",
        "        \"\"\"Handle analysis errors.\"\"\"\n",
        "        print(f\"\\n‚ùå Error: {message}\")\n",
        "        self.report = {\n",
        "            'status': 'error',\n",
        "            'error_message': message,\n",
        "            'file_path': self.file_path,\n",
        "            'analysis_date': datetime.datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def save_report(self, output_path: Optional[str] = None) -> str:\n",
        "        \"\"\"\n",
        "        Save the quality report to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            output_path: Optional custom output path\n",
        "\n",
        "        Returns:\n",
        "            Path to the saved report file\n",
        "        \"\"\"\n",
        "        if not self.report or 'status' in self.report and self.report['status'] == 'error':\n",
        "            print(\"‚ö†Ô∏è  No valid report to save.\")\n",
        "            return \"\"\n",
        "\n",
        "        if not output_path:\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_path = f\"quality_report_{self.file_name}_{timestamp}.json\"\n",
        "\n",
        "        try:\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.report, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"‚úÖ Report saved to: {output_path}\")\n",
        "            return output_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error saving report: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def print_summary(self) -> None:\n",
        "        \"\"\"Print a human-readable summary of the analysis.\"\"\"\n",
        "        if not self.report or 'status' in self.report and self.report['status'] == 'error':\n",
        "            print(\"No analysis results to display.\")\n",
        "            return\n",
        "\n",
        "        report = self.report\n",
        "        summary = report['summary']\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üìä CSV QUALITY ANALYSIS REPORT\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"üìÅ File: {report['metadata']['file_name']}\")\n",
        "        print(f\"üìÖ Analyzed: {report['metadata']['analysis_date'][:10]}\")\n",
        "        print(f\"‚è±Ô∏è  Processing time: {report['metadata']['processing_time_seconds']:.2f}s\")\n",
        "        print(f\"üìà Quality Score: {summary['quality_score']}/100 {summary['quality_rating']}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        print(f\"\\nüìä SUMMARY STATISTICS:\")\n",
        "        print(f\"  ‚Ä¢ Total Rows: {summary['total_rows']:,}\")\n",
        "        print(f\"  ‚Ä¢ Total Columns: {summary['total_columns']}\")\n",
        "        print(f\"  ‚Ä¢ Complete Rows: {summary['complete_rows']:,} ({summary['completeness_percentage']}%)\")\n",
        "\n",
        "        issues = report['issues']\n",
        "        print(f\"\\n‚ö†Ô∏è  IDENTIFIED ISSUES:\")\n",
        "        print(f\"  ‚Ä¢ Duplicate Rows: {issues['duplicates']['count']:,} ({issues['duplicates']['percentage']}%)\")\n",
        "        print(f\"  ‚Ä¢ Invalid Values: {self.metrics.invalid_values:,}\")\n",
        "\n",
        "        print(f\"\\nüîç COLUMN ANALYSIS (Top 5):\")\n",
        "        col_analysis = report['column_analysis']\n",
        "        for i, (col, analysis) in enumerate(list(col_analysis.items())[:5]):\n",
        "            print(f\"  {i+1}. {col}:\")\n",
        "            print(f\"     - Missing: {analysis['missing_count']:,} ({analysis['missing_percentage']}%)\")\n",
        "            print(f\"     - Type: {analysis['data_type']} ({analysis['type_confidence']}% confidence)\")\n",
        "            print(f\"     - Unique Values: {analysis['unique_values']:,}\")\n",
        "\n",
        "        print(f\"\\nüí° RECOMMENDATIONS:\")\n",
        "        for i, rec in enumerate(report['recommendations'][:3], 1):\n",
        "            print(f\"  {i}. {rec}\")\n",
        "\n",
        "        print(f\"\\nüìÅ Report saved as: {self.file_name}_quality_report.json\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the CSV quality analyzer.\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Ironhack Germany - CSV Quality Report Generator\",\n",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
        "        epilog=\"\"\"\n",
        "Examples:\n",
        "  %(prog)s data.csv                    # Analyze a CSV file\n",
        "  %(prog)s data.csv --output my_report # Save with custom name\n",
        "  %(prog)s data.csv --summary-only     # Print summary only\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"file_path\",\n",
        "        help=\"Path to the CSV file to analyze\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-o\", \"--output\",\n",
        "        help=\"Custom output file path for the JSON report\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-s\", \"--summary-only\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Print only the summary, don't save JSON report\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"-q\", \"--quiet\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Quiet mode - minimal output\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(args.file_path):\n",
        "        print(f\"‚ùå Error: File '{args.file_path}' not found.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = CSVQualityAnalyzer(args.file_path)\n",
        "\n",
        "    # Run analysis\n",
        "    report = analyzer.analyze()\n",
        "\n",
        "    if not args.quiet:\n",
        "        analyzer.print_summary()\n",
        "\n",
        "    # Save report unless summary-only mode\n",
        "    if not args.summary_only and report and 'status' not in report:\n",
        "        analyzer.save_report(args.output)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Display Ironhack banner\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"IRONHACK GERMANY - DAY 2 PROJECT\")\n",
        "    print(\"CSV Quality Report Generator\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run the main analysis\n",
        "    main()"
      ],
      "metadata": {
        "id": "05n03oSzcpdX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7cfe06b2-49f4-4023-b52e-7d22d5c0d585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "IRONHACK GERMANY - DAY 2 PROJECT\n",
            "CSV Quality Report Generator\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [-o OUTPUT] [-s] [-q] file_path\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}